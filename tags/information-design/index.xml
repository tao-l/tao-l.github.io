<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Information Design | </title>
    <link>https://tao-l.github.io/tags/information-design/</link>
      <atom:link href="https://tao-l.github.io/tags/information-design/index.xml" rel="self" type="application/rss+xml" />
    <description>Information Design</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 20 Jan 2026 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://tao-l.github.io/media/icon_hu7729264130191091259.png</url>
      <title>Information Design</title>
      <link>https://tao-l.github.io/tags/information-design/</link>
    </image>
    
    <item>
      <title>Information Design with Large Language Models: An Annotated Reading List</title>
      <link>https://tao-l.github.io/pub_journal/lin-llm-info-reading-list-2026/</link>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_journal/lin-llm-info-reading-list-2026/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generalized Principal-Agent Problem with a Learning Agent</title>
      <link>https://tao-l.github.io/pub_journal/lin-generalized-2026/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_journal/lin-generalized-2026/</guid>
      <description>&lt;p&gt;Please see the &lt;a href=&#34;https://tao-l.github.io/publication/lin-generalized-2025&#34;&gt;conference version&lt;/a&gt; of this work for details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Design With Large Language Models</title>
      <link>https://tao-l.github.io/working_papers/info-llm-2025/</link>
      <pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/working_papers/info-llm-2025/</guid>
      <description>&lt;p&gt;Information design is typically studied through the lens of Bayesian signaling, where signals shape beliefs based on their correlation with the true state of the world. However, Behavioral Economics and Psychology emphasize that human decision-making is more complex and can depend on how information is framed. This paper formalizes a language-based notion of framing and bridges this to the popular Bayesian-persuasion model. We model framing as a possibly non-Bayesian, linguistic way to influence a receiver&amp;rsquo;s belief, while a signaling (or recommendation) scheme can further refine this belief in the classic Bayesian way. A key challenge in systematically optimizing in this framework is the vast space of possible framings and the difficulty of predicting their effects on receivers. Based on growing evidence that Large Language Models (LLMs) can effectively serve as proxies for human behavior, we formulate a theoretical model based on access to a framing-to-belief oracle. This model then enables us to precisely characterize when solely optimizing framing or jointly optimizing framing and signaling is tractable. We substantiate our theoretical analysis with an empirical algorithm that leverages LLMs to (1) approximate the framing-to-belief oracle, and (2) optimize over language space using a hill-climbing method. We apply this to two marketing-inspired case studies and validate the effectiveness through analytical and human evaluation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explainable Information Design</title>
      <link>https://tao-l.github.io/working_papers/exp-info-2025/</link>
      <pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/working_papers/exp-info-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generalized Principal-Agent Problem with a Learning Agent</title>
      <link>https://tao-l.github.io/pub_conference/lin-generalized-2025/</link>
      <pubDate>Thu, 24 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/lin-generalized-2025/</guid>
      <description>&lt;p&gt;Classic principal-agent problems such as Stackelberg games, contract design, and Bayesian persuasion, often assume that the agent is able to best respond to the principal&amp;rsquo;s committed strategy.
We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) If the agent uses contextual no-regret learning algorithms with regret $\mathrm{Reg}(T)$, then the principal can guarantee utility at least $U^* - \Theta\big(\sqrt{\tfrac{\mathrm{Reg}(T)}{T}}\big)$, where $U^*$ is the principal&amp;rsquo;s optimal utility in the classic model with a best-responding agent.
(2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret $\mathrm{SReg}(T)$, then the principal cannot obtain utility more than $U^* + O(\frac{\mathrm{SReg(T)}}{T})$.
But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than $U^*$.
These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Design with Unknown Prior</title>
      <link>https://tao-l.github.io/pub_conference/lin-information-2025/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/lin-information-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bias Detection via Signaling</title>
      <link>https://tao-l.github.io/pub_conference/chen-bias-2024/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/chen-bias-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-Sender Persuasion: A Computational Perspective</title>
      <link>https://tao-l.github.io/pub_conference/hossain-multi-sender-2024/</link>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/hossain-multi-sender-2024/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
