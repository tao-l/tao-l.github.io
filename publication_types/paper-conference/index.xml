<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper-Conference | </title>
    <link>https://tao-l.github.io/publication_types/paper-conference/</link>
      <atom:link href="https://tao-l.github.io/publication_types/paper-conference/index.xml" rel="self" type="application/rss+xml" />
    <description>Paper-Conference</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 10 Dec 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://tao-l.github.io/media/icon_hu7729264130191091259.png</url>
      <title>Paper-Conference</title>
      <link>https://tao-l.github.io/publication_types/paper-conference/</link>
    </image>
    
    <item>
      <title>A Unified Approach to Submodular Maximization Under Noise</title>
      <link>https://tao-l.github.io/pub_conference/submodular-2025/</link>
      <pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/submodular-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generalized Principal-Agent Problem with a Learning Agent</title>
      <link>https://tao-l.github.io/pub_conference/lin-generalized-2025/</link>
      <pubDate>Thu, 24 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/lin-generalized-2025/</guid>
      <description>&lt;p&gt;Classic principal-agent problems such as Stackelberg games, contract design, and Bayesian persuasion, often assume that the agent is able to best respond to the principal&amp;rsquo;s committed strategy.
We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) If the agent uses contextual no-regret learning algorithms with regret $\mathrm{Reg}(T)$, then the principal can guarantee utility at least $U^* - \Theta\big(\sqrt{\tfrac{\mathrm{Reg}(T)}{T}}\big)$, where $U^*$ is the principal&amp;rsquo;s optimal utility in the classic model with a best-responding agent.
(2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret $\mathrm{SReg}(T)$, then the principal cannot obtain utility more than $U^* + O(\frac{\mathrm{SReg(T)}}{T})$.
But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than $U^*$.
These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Design with Unknown Prior</title>
      <link>https://tao-l.github.io/pub_conference/lin-information-2025/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/lin-information-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>User-Creator Feature Polarization in Recommender Systems with Dual Influence</title>
      <link>https://tao-l.github.io/pub_conference/lin-user-creator-2024/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/lin-user-creator-2024/</guid>
      <description>&lt;p&gt;Recommender systems serve the dual purpose of presenting relevant content to users and helping content creators reach their target audience. The dual nature of these systems naturally influences both users and creators: users&amp;rsquo; preferences are affected by the items they are recommended, while creators may be incentivized to alter their content to attract more users. We define a model, called user-creator feature dynamics, to capture the dual influence of recommender systems. We prove that a recommender system with dual influence is guaranteed to polarize, causing diversity loss in the system. We then investigate, both theoretically and empirically, approaches for mitigating polarization and promoting diversity in recommender systems. Unexpectedly, we find that common diversity-promoting approaches do not work in the presence of dual influence, while relevancy-optimizing methods like top-$k$ truncation can prevent polarization and improve diversity of the system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bias Detection via Signaling</title>
      <link>https://tao-l.github.io/pub_conference/chen-bias-2024/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/chen-bias-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-Sender Persuasion: A Computational Perspective</title>
      <link>https://tao-l.github.io/pub_conference/hossain-multi-sender-2024/</link>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/hossain-multi-sender-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Thresholds with Latent Values and Censored Feedback</title>
      <link>https://tao-l.github.io/pub_conference/zhang-learning-2024/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/zhang-learning-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sample Complexity of Forecast Aggregation</title>
      <link>https://tao-l.github.io/pub_conference/lin-sample-2023/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/lin-sample-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From Monopoly to Competition: Optimal Contests Prevail</title>
      <link>https://tao-l.github.io/pub_conference/deng-monopoly-2023/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/deng-monopoly-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Nash Convergence of Mean-Based Learning Algorithms in First Price Auctions</title>
      <link>https://tao-l.github.io/pub_conference/deng-nash-2022/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/deng-nash-2022/</guid>
      <description>&lt;p&gt;The convergence properties of learning dynamics in repeated auctions is a timely and important question, with numerous applications in, e.g., online advertising markets.&lt;/p&gt;
&lt;p&gt;This work focuses on repeated first-price auctions where bidders with fixed values learn to bid using mean-based algorithms &amp;mdash; a large class of online learning algorithms that include popular no-regret algorithms such as Multiplicative Weights Update and Follow the Perturbed Leader. We completely characterize the learning dynamics of mean-based algorithms, under two notions of convergence: &lt;strong&gt;(1) time-average:&lt;/strong&gt; the fraction of rounds where bidders play a Nash equilibrium converges to 1; &lt;strong&gt;(2) last-iterate:&lt;/strong&gt; the mixed strategy profile of bidders converges to a Nash equilibrium. Specifically, the results depend on the number of bidders with the highest value:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the number is at least three, the dynamics almost surely converges to a Nash equilibrium of the auction, in both time-average and last-iterate.&lt;/li&gt;
&lt;li&gt;If the number is two, the dynamics almost surely converges to a Nash equilibrium in time-average but not necessarily last-iterate.&lt;/li&gt;
&lt;li&gt;If the number is one, the dynamics may not converge to a Nash equilibrium in time-average or last-iterate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our discovery opens up new possibilities in the study of the convergence of learning dynamics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Many Representatives Do We Need? The Optimal Size of a Congress Voting on Binary Issues</title>
      <link>https://tao-l.github.io/pub_conference/revel-how-2022/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/revel-how-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Utilities and Equilibria in Non-Truthful Auctions</title>
      <link>https://tao-l.github.io/pub_conference/fu-learning-2020/</link>
      <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/fu-learning-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Game-Theoretic Analysis of the Empirical Revenue Maximization Algorithm with Endogenous Sampling</title>
      <link>https://tao-l.github.io/pub_conference/deng-game-theoretic-2020/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/deng-game-theoretic-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Private Data Manipulation in Optimal Sponsored Search Auction</title>
      <link>https://tao-l.github.io/pub_conference/deng-private-2020/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/pub_conference/deng-private-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://tao-l.github.io/example_publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://tao-l.github.io/example_publication/conference-paper/</guid>
      <description>&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;Add the publication&amp;rsquo;s &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; here. You can use rich formatting such as including &lt;a href=&#34;https://docs.hugoblox.com/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
